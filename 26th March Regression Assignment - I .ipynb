{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c269ba",
   "metadata": {},
   "source": [
    "# 26th March 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d323627",
   "metadata": {},
   "source": [
    "## REGRESSION -I Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a9d33",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f91996",
   "metadata": {},
   "source": [
    "* Simple linear regression:\n",
    "    Is a statistical method where the relationship between two variables (a dependent variable and an independent variable) is represented by a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a9c7b",
   "metadata": {},
   "source": [
    "######  * An example of simple linear regression is as follows: Suppose we are analyzing the relationship between temperature and the sales of ice-cream. In this example, we can study how much the increase or decrease in temperature can affect ice-cream sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93e29f0",
   "metadata": {},
   "source": [
    "* multiple linear regression:\n",
    "    Is a statistical method that identifies the linear relationship between more than one independent variable and a dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49a5c2",
   "metadata": {},
   "source": [
    " ###### * An example of multiple linear regression is as follows: Suppose we are analyzing the relationship between the price of a house, the size of a house, and the number of bedrooms in the house. In this example, we can study how much the size of the house, the number of bedrooms, and the price of the house are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c37001",
   "metadata": {},
   "source": [
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1682d7f9",
   "metadata": {},
   "source": [
    "* ASSUMPTIONS:- \n",
    "    * LINEAR RELATIONSHIP: There's a linear relationship between the dependent variable and independent variable.\n",
    "   \n",
    "   #CHECK: Scatter plots: Plot the dependent variable against each independent variable. If the points in the plot form a linear pattern, this indicates linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d96fce",
   "metadata": {},
   "source": [
    "* MULTICOLLINEARITY: There's no multicollinearity present, there should be no correlation among these variable in itself(dont depent on each other).\n",
    "   \n",
    "#CHECK:- Variance inflation factor (VIF): This measures the correlation between each independent variable and all the other independent variables. If any VIF values are high, this indicates multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f61198",
   "metadata": {},
   "source": [
    "* NORMAL RESIDUAL: When you do prediction and the errors came that's know as residual!!his means that the distribution of the residuals should be approximately bell-shaped.\n",
    "    \n",
    "    #CHECK:- Cook's distance: This measures the influence of each observation on the regression coefficients. If there are outliers that have a high Cook's distance, this indicates that they may be influential and should be investigated further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c83e0b",
   "metadata": {},
   "source": [
    "* HOMOSCEDASTICITY: MEANS(same spread), \"having the same scatter\", when you plot your residual so its spread should be equal,\n",
    "    if not equal than known as 'Heteroscedasticity'\n",
    "    \n",
    "    \n",
    "    #CHECK: Normal probability plots: Plot the residuals against a normal distribution. If the plot shows a straight line, this indicates normality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54ee23",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8873d",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept represent the relationship between the independent variable(s) and the dependent variable.\n",
    "\n",
    "The intercept (often denoted as β₀ or b₀) represents the value of the dependent variable when all independent variables are set to zero. It indicates the baseline or starting point of the relationship. In other words, it represents the expected value of the dependent variable when the independent variable(s) have no effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0957e",
   "metadata": {},
   "source": [
    "The slope (often denoted as β₁ or b₁) represents the change in the dependent variable associated with a one-unit change in the independent variable. It indicates the rate of change in the dependent variable for each unit change in the independent variable.\n",
    "\n",
    "##### an example using a real-world scenario:\n",
    "Suppose we want to analyze the relationship between the number of hours studied and the score obtained in a math exam. We collect data from a group of students and perform a linear regression analysis. The resulting model is:\n",
    "\n",
    "Score = 60 + 5 * Hours\n",
    "Therefore, in \n",
    "In this example, the intercept is 60, which means that if a student does not study at all (0 hours), the expected score would be 60.\n",
    "\n",
    "The slope is 5, which indicates that for every additional hour of studying, the expected score increases by 5 points. So, if a student studies for 3 hours, the expected score would be 60 + 5 * 3 = 75.\n",
    "\n",
    "Therefore, in this scenario, the intercept represents the expected score for a student who does not study, while the slope represents the increase in the expected score for each additional hour of studying.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c4f89a",
   "metadata": {},
   "source": [
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb909d8e",
   "metadata": {},
   "source": [
    "\"Gradient descent is an optimization algorithm commonly used in machine learning for finding the minimum of a function\"\n",
    "It is particularly useful when dealing with models that have a large number of parameters and complex relationships.\n",
    "\n",
    "* Objective Function: In machine learning, we often aim to minimize an objective function, also known as a cost function or loss function. This function quantifies the error or mismatch between the predicted outputs of a model and the actual observed values.\n",
    "\n",
    "* Parameter Space: The objective function depends on a set of parameters that define the model. These parameters can be adjusted to minimize the objective function and improve the model's performance.\n",
    "\n",
    "* Gradient: The gradient of a function is a vector that indicates the direction of the steepest ascent or descent. It points toward the direction of the maximum increase or decrease of the function.\n",
    "\n",
    "* Minimization: The goal of gradient descent is to iteratively update the parameter values in a way that minimizes the objective function. It does this by repeatedly computing the gradient of the objective function with respect to the parameters and adjusting the parameters in the opposite direction of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5be678",
   "metadata": {},
   "source": [
    "#The steps of the gradient descent algorithm can be summarized as follows:\n",
    "\n",
    "Initialize Parameters: Start with an initial set of parameter values.\n",
    "\n",
    "Compute Gradient: Calculate the gradient of the objective function with respect to the parameters. This involves taking partial derivatives of the objective function with respect to each parameter.\n",
    "\n",
    "Update Parameters: Adjust the parameter values by moving in the opposite direction of the gradient. This is done by multiplying the gradient by a learning rate, which determines the step size for each iteration.\n",
    "\n",
    "Repeat: Repeat steps 2 and 3 until convergence or a specified number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e423db78",
   "metadata": {},
   "source": [
    "* Gradient descent helps in finding the optimal parameter values that minimize the objective function and improve the model's performance. \n",
    "\n",
    "* It is widely used in machine learning algorithms such as linear regression, logistic regression, neural networks, and deep learning models. \n",
    "\n",
    "* By iteratively updating the parameters based on the gradient, the algorithm can navigate the parameter space to find the values that lead to the best fit of the model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc413c7",
   "metadata": {},
   "source": [
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a92af5",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression that allows for the analysis of the relationship between a dependent variable and multiple independent variables. While simple linear regression involves only one independent variable, multiple linear regression can accommodate two or more independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5a999",
   "metadata": {},
   "source": [
    "The multiple linear regression model can be represented by the following equation:\n",
    "\n",
    "Y = β₀ + β₁X₁ + β₂X₂ + ... + βₚ*Xₚ + ε"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2668560",
   "metadata": {},
   "source": [
    "In this equation:\n",
    "\n",
    "* Y represents the dependent variable (the variable we want to predict or explain).\n",
    "\n",
    "* X₁, X₂, ..., Xₚ are the independent variables (also known as predictors or features).\n",
    "\n",
    "* β₀, β₁, β₂, ..., βₚ are the regression coefficients that represent the expected change in the dependent variable for a one-unit change in each independent variable, assuming the other variables are held constant.\n",
    "\n",
    "* ε represents the error term, which captures the variability in the dependent variable that is not explained by the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbdc9db",
   "metadata": {},
   "source": [
    "The key differences between multiple linear regression and simple linear regression are:\n",
    "    \n",
    "    * Number of Independent Variables: Simple linear regression involves only one independent variable, while multiple linear regression allows for the inclusion of multiple independent variables. \n",
    "        \n",
    "        * Interpretation of Coefficients: In simple linear regression, the slope coefficient represents the change in the dependent variable for a one-unit change in the independent variable. \n",
    "            \n",
    "            * Model Complexity: Multiple linear regression is a more complex model than simple linear regression due to the inclusion of multiple independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5a862",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d72ad",
   "metadata": {},
   "source": [
    "Multicollinearity refers to a situation in multiple linear regression where two or more independent variables are highly correlated with each other. It can cause problems in the regression analysis by inflating the standard errors of the regression coefficients, making them imprecise and potentially leading to unreliable interpretations of the effects of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564c805",
   "metadata": {},
   "source": [
    "Detecting Multicollinearity:\n",
    "There are several methods to detect multicollinearity:\n",
    "\n",
    "* Correlation Matrix: Calculate the correlation coefficients between all pairs of independent variables. Correlation coefficients close to +1 or -1 indicate high correlation.\n",
    "\n",
    "* Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF measures how much the variance of the estimated regression coefficient is inflated due to multicollinearity. VIF values greater than 1 indicate potential multicollinearity, and values above 5 or 10 are often considered problematic.\n",
    "\n",
    "* Tolerance: Calculate the tolerance value for each independent variable, which is the reciprocal of the VIF. Tolerance values close to 1 indicate low multicollinearity, while values close to 0 indicate high multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73259d",
   "metadata": {},
   "source": [
    "Addressing Multicollinearity:\n",
    "If multicollinearity is detected, several approaches can be used to address the issue:\n",
    "\n",
    "* Variable Selection: Remove one or more highly correlated independent variables from the model. By eliminating redundant variables, you can reduce multicollinearity and improve the stability of the regression coefficients. However, be cautious not to remove variables that are theoretically or substantively important.\n",
    "\n",
    "* Data Collection: Collect more data to increase the sample size. Multicollinearity can be partly caused by insufficient data, and a larger sample size can help reduce the correlation between variables.\n",
    "\n",
    "* Principal Component Analysis (PCA): Perform PCA to transform the original set of correlated variables into a smaller set of uncorrelated variables (principal components). The principal components can then be used as the independent variables in the regression analysis, avoiding multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c7a39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f923003a",
   "metadata": {},
   "source": [
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb876600",
   "metadata": {},
   "source": [
    "\"Polynomial regression is an extension of linear regression that allows for capturing non-linear relationships between the dependent variable and the independent variable(s). While linear regression assumes a linear relationship, polynomial regression can model curved or non-linear relationships by including polynomial terms of the independent variable(s) in the model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d383c03",
   "metadata": {},
   "source": [
    "The polynomial regression model can be represented by the following equation:\n",
    "\n",
    "Y = β₀ + β₁X + β₂X² + ... + βₚ*Xᵣ + ε\n",
    "\n",
    "In this equation:\n",
    "\n",
    "* Y represents the dependent variable.\n",
    "\n",
    "* X represents the independent variable.\n",
    "\n",
    "* X², X³, ..., Xᵣ are the polynomial terms of the independent variable, each raised to a different power (2, 3, ..., ᵣ).\n",
    "\n",
    "* β₀, β₁, β₂, ..., βₚ are the regression coefficients representing the impact of each term on the dependent variable.\n",
    "\n",
    "* ε represents the error term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fbfe92",
   "metadata": {},
   "source": [
    "The key differences between polynomial regression and linear regression are:\n",
    "    \n",
    "    * Modeling Non-linearity: Polynomial regression allows for capturing non-linear relationships between the dependent variable and the independent variable. \n",
    "        \n",
    "        * Flexibility: While linear regression assumes a linear relationship, polynomial regression provides more flexibility by allowing the model to fit curves to the data.\n",
    "            \n",
    "         * Model Complexity: Polynomial regression can introduce more complexity compared to linear regression.\n",
    "                \n",
    "         * Interpretation: In linear regression, the coefficients represent the change in the dependent variable for a one-unit change in the independent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4658875c",
   "metadata": {},
   "source": [
    "'Polynomial regression is commonly used in situations where the relationship between the variables is non-linear, and there is evidence of curvature or other non-linear patterns in the data. By allowing for more flexible modeling, polynomial regression can better capture the complexity of the underlying relationships and improve the accuracy of predictions.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d16a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c39b6a79",
   "metadata": {},
   "source": [
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74acb22a",
   "metadata": {},
   "source": [
    "Advantages of Polynomial Regression:\n",
    "\n",
    "* Capturing Non-linear Relationships: Polynomial regression can capture non-linear relationships between the dependent variable and independent variable(s). By including polynomial terms, it can model curved or non-linear patterns in the data more accurately than linear regression.\n",
    "\n",
    "* Flexibility: Polynomial regression provides more flexibility in fitting the data by allowing for curves and interactions between variables. It can capture more complex relationships and variations in the data that linear regression may not be able to capture effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3950f",
   "metadata": {},
   "source": [
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "* Increased Complexity: As the degree of the polynomial increases, the model becomes more complex. Higher-order polynomials can introduce more parameters, which may lead to overfitting the training data and decreased generalization to new data.\n",
    "\n",
    "* Interpretability: The interpretation of coefficients becomes more complicated in polynomial regression. Each coefficient corresponds to the impact of a specific term, and the relationship between the variables can be more challenging to interpret and explain compared to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280cbb3",
   "metadata": {},
   "source": [
    "Situations to Prefer Polynomial Regression:\n",
    "Polynomial regression is particularly useful in the following situations:\n",
    "\n",
    "* Non-linear Relationships: When there is evidence of non-linear relationships between the variables, polynomial regression can better capture the underlying patterns and improve the model's fit to the data.\n",
    "\n",
    "* Curved Patterns: If the data exhibits curved patterns, polynomial regression can accurately model these curves and provide better predictions compared to linear regression.\n",
    "\n",
    "* Higher Flexibility Requirement: When linear regression fails to capture the complexity of the relationship between the variables, polynomial regression offers greater flexibility by including polynomial terms and allowing for interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f290338",
   "metadata": {},
   "source": [
    "## THANK YOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b2cf3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
